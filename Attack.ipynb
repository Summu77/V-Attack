{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, InterpolationMode\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "from open_clip.constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "\n",
    "\n",
    "def load_csv_data(csv_path):\n",
    "    data = []\n",
    "    with open(csv_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            row['entities'] = [e.strip() for e in row['entities'].split(',')]\n",
    "            data.append(row)\n",
    "    return data  \n",
    "\n",
    "device = \"cuda:3\" \n",
    "clipmodel, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "\n",
    "# Load data\n",
    "\n",
    "mean = OPENAI_DATASET_MEAN\n",
    "std = OPENAI_DATASET_STD\n",
    "processor_before = transforms.Compose([transforms.Resize(size=(336, 336), interpolation=transforms.InterpolationMode.BICUBIC),transforms.ToTensor(),])\n",
    "processor_after = transforms.Compose([transforms.Normalize(mean, std),])\n",
    "\n",
    "\n",
    "clip_inres = clipmodel.visual.input_resolution\n",
    "clip_ksize = clipmodel.visual.conv1.kernel_size\n",
    "def attention_layer(q, k, v, num_heads=1):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    tgt_len, bsz, embed_dim = q.shape\n",
    "    head_dim = embed_dim // num_heads\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "    q = q * scaling\n",
    "    \n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "    attn_output_heads = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output_heads.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output = attn_output_heads.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, -1)\n",
    "    attn_output_weights = attn_output_weights.sum(dim=1) / num_heads\n",
    "    return attn_output, attn_output_weights\n",
    "    \n",
    "def clip_encode_dense(x,n):\n",
    "    vision_width = clipmodel.visual.transformer.width\n",
    "    vision_heads = vision_width // 64\n",
    "    # print(\"[vision_width and vision_heads]:\", vision_width, vision_heads) #[vision_width and vision_heads]: 1024 16\n",
    "    \n",
    "    # modified from CLIP\n",
    "    x = x.half()\n",
    "    x = clipmodel.visual.conv1(x) \n",
    "    feah, feaw = x.shape[-2:]\n",
    "\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) \n",
    "    x = x.permute(0, 2, 1) \n",
    "    class_embedding = clipmodel.visual.class_embedding.to(x.dtype)\n",
    "    x = torch.cat([class_embedding + torch.zeros(x.shape[0], 1, x.shape[-1]).to(x), x], dim=1) # 加入类别嵌入\n",
    "\n",
    "    pos_embedding = clipmodel.visual.positional_embedding.to(x.dtype)\n",
    "    tok_pos, img_pos = pos_embedding[:1, :], pos_embedding[1:, :]\n",
    "    pos_h = clip_inres // clip_ksize[0]\n",
    "    pos_w = clip_inres // clip_ksize[1]\n",
    "    assert img_pos.size(0) == (pos_h * pos_w), f\"the size of pos_embedding ({img_pos.size(0)}) does not match resolution shape pos_h ({pos_h}) * pos_w ({pos_w})\"\n",
    "    img_pos = img_pos.reshape(1, pos_h, pos_w, img_pos.shape[1]).permute(0, 3, 1, 2)\n",
    "\n",
    "    img_pos = torch.nn.functional.interpolate(img_pos, size=(feah, feaw), mode='bicubic', align_corners=False)\n",
    "    img_pos = img_pos.reshape(1, img_pos.shape[1], -1).permute(0, 2, 1)\n",
    "    pos_embedding = torch.cat((tok_pos[None, ...], img_pos), dim=1)\n",
    "    x = x + pos_embedding\n",
    "\n",
    "    x = clipmodel.visual.ln_pre(x)\n",
    "    \n",
    "    x = x.permute(1, 0, 2)  \n",
    "\n",
    "    x = torch.nn.Sequential(*clipmodel.visual.transformer.resblocks[:-n])(x)\n",
    "\n",
    "    attns = []\n",
    "    atten_outs = []\n",
    "    vs = []\n",
    "    qs = []\n",
    "    ks = []\n",
    "    x_in_list = []\n",
    "    x_out_list = []\n",
    "\n",
    "    for TR in clipmodel.visual.transformer.resblocks[-n:]:\n",
    "        x_in = x\n",
    "        x = TR.ln_1(x_in)\n",
    "        x_in_list.append(x)\n",
    "        linear = torch._C._nn.linear    \n",
    "        q, k, v = linear(x, TR.attn.in_proj_weight, TR.attn.in_proj_bias).chunk(3, dim=-1)\n",
    "        attn_output, attn = attention_layer(q, k, v, vision_heads)  \n",
    "        attns.append(attn)\n",
    "        atten_outs.append(attn_output)\n",
    "        vs.append(v)\n",
    "        qs.append(q)\n",
    "        ks.append(k)\n",
    "        \n",
    "        x_after_attn = linear(attn_output, TR.attn.out_proj.weight, TR.attn.out_proj.bias)     \n",
    "        x = x_after_attn + x_in\n",
    "        x = x + TR.mlp(TR.ln_2(x))\n",
    "        ww = x.permute(1, 0, 2)\n",
    "        ww = clipmodel.visual.ln_post(ww)\n",
    "        ww = ww @ clipmodel.visual.proj\n",
    "        x_out_list.append(ww)\n",
    "\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "    x = clipmodel.visual.ln_post(x)\n",
    "    x = x @ clipmodel.visual.proj\n",
    "    return x, x_in_list, vs, qs, ks, attns, atten_outs, (feah, feaw)\n",
    "\n",
    "def save_image(image, output_path):\n",
    "    # Convert to PIL image and save\n",
    "    img_np = image.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n",
    "    img_pil = Image.fromarray(img_np)\n",
    "    img_pil.save(output_path)\n",
    "\n",
    "def plot_adv_ori(input_img, output_adv):\n",
    "    original_img = input_img.cpu() \n",
    "    adversarial_img = output_adv.cpu()  \n",
    "\n",
    "    original_img = original_img.squeeze(0)\n",
    "    adversarial_img = adversarial_img.squeeze(0)\n",
    "\n",
    "    original_img = original_img.permute(1, 2, 0)\n",
    "    adversarial_img = adversarial_img.permute(1, 2, 0)\n",
    "\n",
    "    original_img = original_img.numpy()\n",
    "    adversarial_img = adversarial_img.numpy()\n",
    "\n",
    "    difference = np.abs(original_img - adversarial_img) * 3\n",
    "\n",
    "    difference_normalized = (difference - difference.min()) / (difference.max() - difference.min())\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(adversarial_img)\n",
    "    plt.title('Adversarial Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(difference_normalized, cmap='hot')  \n",
    "    plt.title('Difference (Original vs Adversarial)')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_text_feat(text, model, device):\n",
    "\n",
    "    ## ori_text_feat\n",
    "    tokenizer = open_clip.get_tokenizer(model_name='ViT-L-14-336')\n",
    "    prompts = tokenizer(text).to(device)\n",
    "    text_embedding = model.encode_text(prompts)\n",
    "    text_embedding = F.normalize(text_embedding, dim=-1)\n",
    "    text_feat = text_embedding.unsqueeze(0)# [1, num_prompt, 768]\n",
    "\n",
    "    return text_feat\n",
    "\n",
    "def get_mask_and_feat(model, text_feat, ori_vs, ori_ks, if_plot = False):\n",
    "\n",
    "    # get ori_feat_list\n",
    "    ori_feat_list = [] \n",
    "    for j in range(0, 24):\n",
    "        ww = ori_vs[j]\n",
    "        linear = torch._C._nn.linear\n",
    "        TR = model.visual.transformer.resblocks[j]\n",
    "        ww = linear(ww, TR.attn.out_proj.weight, TR.attn.out_proj.bias)\n",
    "        ww = ww.permute(1, 0, 2)  \n",
    "        ww = model.visual.ln_post(ww)\n",
    "        ww = ww @ model.visual.proj\n",
    "        ori_feat = F.normalize(ww, dim=-1) # [1, num_patch, 768]\n",
    "        ori_feat_list.append(ori_feat)\n",
    "    \n",
    "    # get mask\n",
    "    xx = ori_vs[-1]\n",
    "    linear = torch._C._nn.linear\n",
    "    TR = model.visual.transformer.resblocks[-1]\n",
    "    for i in range(20):\n",
    "        xx , _ =  attention_layer(ori_ks[8], ori_ks[8], xx , 16)\n",
    "    xx = linear(xx, TR.attn.out_proj.weight, TR.attn.out_proj.bias)\n",
    "    xx = xx.permute(1, 0, 2)  \n",
    "    xx = model.visual.ln_post(xx)\n",
    "    xx = xx @ model.visual.proj\n",
    "    ori_feat = F.normalize(xx, dim=-1)\n",
    "\n",
    "    img_txt_matching = ori_feat[:, 1:, :] @ text_feat.transpose(-1, -2)\n",
    "    mask_list = []\n",
    "    xmask_list = []\n",
    "    for i in range(len(text)):\n",
    "        if len(text)==1:\n",
    "            x = img_txt_matching.squeeze()\n",
    "        else:\n",
    "            x = img_txt_matching.squeeze()[:,i]\n",
    "        threshold = (min(x) + max(x))/2 \n",
    "        \n",
    "        # gen mask\n",
    "        xmask = x.reshape(24, 24)\n",
    "        mask = (xmask > threshold).float()\n",
    "        \n",
    "        xmask_resized = F.interpolate(\n",
    "            xmask.unsqueeze(0).unsqueeze(0),  \n",
    "            size=(336, 336),\n",
    "            mode='bilinear',\n",
    "            align_corners= False\n",
    "        ).squeeze() \n",
    "        xmask_list.append(xmask_resized)\n",
    "\n",
    "        mask_resized = F.interpolate(\n",
    "            mask.unsqueeze(0).unsqueeze(0),  \n",
    "            size=(336, 336),\n",
    "            mode=\"nearest\"  ,\n",
    "            align_corners= None\n",
    "        ).squeeze() \n",
    "        mask_list.append(mask_resized)\n",
    "        \n",
    "        # plot mask\n",
    "        if if_plot:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            im1 = ax1.imshow(x.reshape(24, 24).detach().cpu().numpy())\n",
    "            ax1.set_title('Original Image (24x24)')\n",
    "            ax1.axis('off')\n",
    "            fig.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "            im2 = ax2.imshow(mask_resized.cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "            ax2.set_title('Resized Mask (336x336)')\n",
    "            ax2.axis('off')\n",
    "            fig.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return ori_feat_list, mask_list, xmask_list\n",
    "        \n",
    "def random_crop_within_bounds(X, mask, xmask, min_scale=0.75, max_scale=1.0):\n",
    "\n",
    "    img_height, img_width = X.size(1), X.size(2)\n",
    "\n",
    "    if xmask.numel() > 0:\n",
    "        k = int(0.15 * xmask.numel())\n",
    "        if k == 0:\n",
    "            k = 1\n",
    "        top_values = torch.topk(xmask.flatten(), k).values\n",
    "        threshold = top_values[-1]\n",
    "        important_mask = xmask >= threshold\n",
    "    else:\n",
    "        important_mask = torch.zeros_like(xmask, dtype=torch.bool)\n",
    "    \n",
    "\n",
    "    rows = torch.any(important_mask, dim=1)\n",
    "    cols = torch.any(important_mask, dim=0)\n",
    "    \n",
    "    if torch.any(rows) and torch.any(cols):\n",
    "\n",
    "        min_row, max_row = torch.where(rows)[0][[0, -1]]\n",
    "        min_col, max_col = torch.where(cols)[0][[0, -1]]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        rows = torch.any(mask, dim=1)\n",
    "        cols = torch.any(mask, dim=0)\n",
    "        min_row, max_row = torch.where(rows)[0][[0, -1]]\n",
    "        min_col, max_col = torch.where(cols)[0][[0, -1]]\n",
    "\n",
    "    min_height = max_row - min_row + 1\n",
    "    min_width = max_col - min_col + 1\n",
    "\n",
    "    crop_height = int(img_height * random.uniform(min_scale, max_scale))\n",
    "    crop_width = int(img_width * random.uniform(min_scale, max_scale))\n",
    "\n",
    "    crop_height = min(crop_height, img_height)\n",
    "    crop_width = min(crop_width, img_width)\n",
    "\n",
    "    crop_height = max(crop_height, min_height)\n",
    "    crop_width = max(crop_width, min_width)\n",
    "\n",
    "    max_top = min(min_row, img_height - crop_height)\n",
    "    max_left = min(min_col, img_width - crop_width)\n",
    "\n",
    "    max_top = max(0, max_top)\n",
    "    max_left = max(0, max_left)\n",
    "    \n",
    "\n",
    "    top_range_min = max(0, min_row - (crop_height - min_height))\n",
    "    top_range_max = max_top\n",
    "    left_range_min = max(0, min_col - (crop_width - min_width))\n",
    "    left_range_max = max_left\n",
    "    \n",
    "\n",
    "    if top_range_min == top_range_max and left_range_min == left_range_max:\n",
    "        crop_top = top_range_min\n",
    "        crop_left = left_range_min\n",
    "    else:\n",
    "        crop_top = random.randint(top_range_min, top_range_max)\n",
    "        crop_left = random.randint(left_range_min, left_range_max)\n",
    "\n",
    "    cropped_X = X[:,\n",
    "                 crop_top:crop_top + crop_height,\n",
    "                 crop_left:crop_left + crop_width]\n",
    "    \n",
    "    cropped_mask = mask[crop_top:crop_top + crop_height,\n",
    "                       crop_left:crop_left + crop_width]\n",
    "    \n",
    "    return cropped_X, cropped_mask, (crop_top, crop_left, crop_height, crop_width), (min_height, min_width)\n",
    "\n",
    "\n",
    "def pgd_attack(model, processor, image, text, epsilon, alpha, num_iter, device):\n",
    "\n",
    "    text_feat = get_text_feat(text, model, device)\n",
    "\n",
    "    ori_image = image.clone().detach() # [3, 448, 448]\n",
    "    with torch.no_grad():\n",
    "        ori_feat, ori_n_block_inputs, ori_vs, ori_qs, ori_ks, ori_attns, ori_atten_outs, ori_map_size = clip_encode_dense(processor(ori_image).unsqueeze(0).to(device), n=24)\n",
    "        ori_feat_list, mask_list, xmask_list = get_mask_and_feat(model, text_feat, ori_vs, ori_ks)\n",
    "        \n",
    "    adv_image = image.clone().detach() + torch.from_numpy(np.random.uniform(-alpha, alpha, image.shape)).float()\n",
    "\n",
    "    adv_vs_cash = ori_vs\n",
    "    for i in range(num_iter):\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ori_feat_list, mask_list, xmask_list = get_mask_and_feat(model, text_feat, adv_vs_cash, ori_ks, False) \n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        input_mask = mask_list[0]\n",
    "        input_xmask = xmask_list[0]\n",
    "            \n",
    "        cropped_x, cropped_mask, (crop_top, crop_left, crop_height, crop_width), (min_height, min_width) = random_crop_within_bounds(adv_image, input_mask, input_xmask)\n",
    "\n",
    "\n",
    "        x = F.interpolate(cropped_x.unsqueeze(0), size=(336, 336), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        mask = F.interpolate(cropped_mask.unsqueeze(0).unsqueeze(0), size=(24, 24), mode='nearest').squeeze().flatten()\n",
    "\n",
    "        x = processor(x.to('cpu')).to(device).requires_grad_(True)\n",
    "        adv_feat, adv_n_block_inputs, adv_vs, adv_qs, adv_ks, adv_attns, adv_atten_outs, adv_map_size = clip_encode_dense(x.unsqueeze(0), n=24) \n",
    "        adv_vs_cash = adv_vs \n",
    "\n",
    "        for j in range(20,24):\n",
    "            xx = adv_vs[j]\n",
    "            TR = model.visual.transformer.resblocks[j]\n",
    "            linear = torch._C._nn.linear\n",
    "            xx = linear(xx, TR.attn.out_proj.weight, TR.attn.out_proj.bias)\n",
    "            xx = xx.permute(1, 0, 2)  \n",
    "            xx = model.visual.ln_post(xx)\n",
    "            xx = xx @ model.visual.proj\n",
    "            adv_feat_V = F.normalize(xx, dim=-1)\n",
    "\n",
    "            # semantic loss\n",
    "            for k in  [1]:\n",
    "                semantic_loss = F.cosine_similarity(text_feat[:,k,:].unsqueeze(0), adv_feat_V[:,1:,:], dim=-1).squeeze()\n",
    "                semantic_loss = semantic_loss * mask\n",
    "                loss += torch.sum(semantic_loss)\n",
    "            for kk in [0]:\n",
    "                semantic_loss = - F.cosine_similarity(text_feat[:,kk,:].unsqueeze(0), adv_feat_V[:,1:,:], dim=-1).squeeze()\n",
    "                semantic_loss = semantic_loss * mask\n",
    "                loss += torch.sum(semantic_loss)\n",
    "\n",
    "\n",
    "        model.visual.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            original_noise = torch.zeros_like(adv_image)\n",
    "            crop_noise =  F.interpolate((alpha * x.grad.sign()).unsqueeze(0), \n",
    "                                      size=(crop_height, crop_width), \n",
    "                                      mode='bilinear', \n",
    "                                      align_corners=False).squeeze(0)\n",
    "            \n",
    "            original_noise[:, crop_top:crop_top + crop_height, crop_left:crop_left + crop_width] = crop_noise\n",
    "            adv_image = adv_image.to(device) + original_noise.to(device)\n",
    "            delta = torch.clamp(adv_image - ori_image.to(device), -epsilon, epsilon)\n",
    "            adv_image = torch.clamp(ori_image.to(device) + delta, min=0, max=1).detach_()\n",
    "\n",
    "    return ori_image, adv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "json_path = 'coco300_object_descriptions_main.json'\n",
    "image_dir = 'coco_dataset/val2017'  \n",
    "target_path = 'coco300_target_obj_sim.json'\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "with open(target_path, 'r') as f:\n",
    "    tar_data = json.load(f)\n",
    "\n",
    "for i in range(0,1):\n",
    "    # Get text and image\n",
    "    k = i\n",
    "    print(k)\n",
    "    item = all_data[k]\n",
    "    image_path = item['image']\n",
    "    captions = item['caption']\n",
    "\n",
    "    target = tar_data[k]['replace'][0][:tar_data[k]['replace'][0].find(':')].strip()\n",
    "    target_caption = tar_data[k]['replace'][0][tar_data[k]['replace'][0].find(':') + 1:].strip()\n",
    "\n",
    "    path_image = os.path.join(image_dir, image_path)\n",
    "    img = Image.open(path_image).convert(\"RGB\")\n",
    "\n",
    "    split_index = captions[0].find(':')\n",
    "    name = captions[0][:split_index].strip()\n",
    "    caption = captions[0][split_index+1:].strip()\n",
    "    text = [name, target]\n",
    "\n",
    "    image = processor_before(img)\n",
    "    start_time = time.time()\n",
    "\n",
    "    ori_image, adv_image = pgd_attack(model=clipmodel, processor=processor_after, image=image, text=text, \n",
    "                                    epsilon=16/255, alpha=3/255, num_iter=200, device=device)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "    # Save images\n",
    "    adv_output_dir = \"\"\n",
    "    os.makedirs(adv_output_dir, exist_ok=True)\n",
    "    adv_output_path = os.path.join(adv_output_dir, f\"{os.path.basename(path_image)}\")\n",
    "    save_image(adv_image, adv_output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attackvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
