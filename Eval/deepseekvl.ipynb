{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
    "from deepseek_vl.utils.io import load_pil_images\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/deepseek-vl-7b-chat\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).to('cuda:1').eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "adv_dir = \"...\"\n",
    "vqa_json_path = \"coco300_vqa_main.json\"\n",
    "output_json = \"deepseek_eval_res.json\" \n",
    "\n",
    "with open(vqa_json_path, 'r') as f:\n",
    "    vqa_data = json.load(f)\n",
    "\n",
    "image_to_questions = {item['image']: item['vqa'] for item in vqa_data}\n",
    "\n",
    "adv_files = sorted(glob(os.path.join(adv_dir, \"*.png\")))\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, adv_path in enumerate(adv_files):\n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "    filename = os.path.basename(adv_path)\n",
    "    # .replace(\".png\", \".jpg\")\n",
    "    \n",
    "    questions = image_to_questions.get(filename, [\"\", \"\", \"\"])\n",
    "    if len(questions) != 3:\n",
    "        questions = questions[:3] + [\"\"] * (3 - len(questions))\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>Describe this image in one sentence.\",\n",
    "            \"images\": [adv_path]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation,\n",
    "        images=pil_images,\n",
    "        force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "    \n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "    \n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    adv_response_1 = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    print(f'ADV [{filename}] Q1: {adv_response_1}')\n",
    "    \n",
    "    adv_responses = [adv_response_1]\n",
    "    for i, question_text in enumerate(questions, start=2):\n",
    "\n",
    "        conversation[0][\"content\"] = f\"<image_placeholder>{question_text}\"\n",
    "\n",
    "        pil_images = load_pil_images(conversation)\n",
    "        prepare_inputs = vl_chat_processor(\n",
    "            conversations=conversation,\n",
    "            images=pil_images,\n",
    "            force_batchify=True\n",
    "        ).to(vl_gpt.device)\n",
    "        \n",
    "        inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "        \n",
    "        outputs = vl_gpt.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepare_inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        \n",
    "        adv_response = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "        print(f'ADV [{filename}] Q{i}: {adv_response}')\n",
    "        \n",
    "        adv_responses.append(adv_response)\n",
    "\n",
    "    results.append({\n",
    "        \"filename\": filename,\n",
    "        \"adversarial_response_1\": adv_responses[0],\n",
    "        \"adversarial_response_2\": adv_responses[1] if len(adv_responses) > 1 else \"\",\n",
    "        \"adversarial_response_3\": adv_responses[2] if len(adv_responses) > 2 else \"\",\n",
    "        \"adversarial_response_4\": adv_responses[3] if len(adv_responses) > 3 else \"\"\n",
    "    })\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
