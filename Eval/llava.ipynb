{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "image_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llava\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_id = '/mnt/sdb1/niesen/model/llava-hf/llava-1.5-7b-hf'\n",
    "\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "llava_processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize conversation template for the description\n",
    "description_conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image briefly in one sentence.\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Set paths and parameters\n",
    "adv_image_dir = '...'\n",
    "vqa_questions_path = 'coco300_vqa_main.json'\n",
    "output_json = \"llava_eval_res.json\"\n",
    "\n",
    "# Load VQA questions\n",
    "with open(vqa_questions_path, 'r') as f:\n",
    "    vqa_data = json.load(f)\n",
    "\n",
    "# Create a mapping from filename to questions for quick lookup\n",
    "questions_map = {item['image']: item['vqa'] for item in vqa_data}\n",
    "\n",
    "# Function to create conversation for a specific question\n",
    "def create_question_conversation(question):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# Load adversarial images\n",
    "adv_images = []\n",
    "adv_files = []\n",
    "for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "    adv_files.extend(glob(os.path.join(adv_image_dir, ext)))\n",
    "adv_files.sort()\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, adv_file in enumerate(adv_files):\n",
    "        if i == 100:\n",
    "            break\n",
    "        filename = os.path.basename(adv_file).replace(\".png\", \".jpg\")\n",
    "        print(f'Processing: {filename}')\n",
    "        \n",
    "        # Skip if we don't have questions for this image\n",
    "        if filename not in questions_map:\n",
    "            print(f\"Skipping {filename} - no questions found\")\n",
    "            continue\n",
    "            \n",
    "        # Get the questions for this image\n",
    "        questions = questions_map[filename]\n",
    "        if len(questions) < 3:\n",
    "            print(f\"Warning: Only {len(questions)} questions found for {filename}\")\n",
    "            questions.extend([\"\"] * (3 - len(questions)))  # Pad with empty questions if needed\n",
    "            \n",
    "        # Load the adversarial image\n",
    "        adv_image = Image.open(adv_file).convert('RGB')\n",
    "        \n",
    "        # Process description\n",
    "        text = llava_processor.apply_chat_template(description_conversation, add_generation_prompt=True)\n",
    "        inputs = llava_processor(images=adv_image, text=text, return_tensors='pt').to(device, torch.float16)\n",
    "        output = llava_model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "        begin_token = inputs['input_ids'].shape[1]\n",
    "        description = llava_processor.decode(output[0][begin_token:], skip_special_tokens=True)\n",
    "        \n",
    "        # Process each question\n",
    "        question_responses = []\n",
    "        for question in questions[:3]:  # Only take first 3 questions\n",
    "            if not question.strip():  # Skip empty questions\n",
    "                question_responses.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            q_conversation = create_question_conversation(question)\n",
    "            q_text = llava_processor.apply_chat_template(q_conversation, add_generation_prompt=True)\n",
    "            q_inputs = llava_processor(images=adv_image, text=q_text, return_tensors='pt').to(device, torch.float16)\n",
    "            q_output = llava_model.generate(**q_inputs, max_new_tokens=200, do_sample=False)\n",
    "            q_begin_token = q_inputs['input_ids'].shape[1]\n",
    "            q_response = llava_processor.decode(q_output[0][q_begin_token:], skip_special_tokens=True)\n",
    "            question_responses.append(q_response)\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"adversarial_response_1\": description,\n",
    "            \"adversarial_response_2\": question_responses[0],\n",
    "            \"adversarial_response_3\": question_responses[1],\n",
    "            \"adversarial_response_4\": question_responses[2],\n",
    "        })\n",
    "\n",
    "# Save as JSON file\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attackvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
